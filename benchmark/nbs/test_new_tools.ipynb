{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "import requests\n",
    "import html2text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from readability import Document\n",
    "from docstring_parser import parse\n",
    "from pydantic import BaseModel, Field\n",
    "from duckduckgo_search import DDGS\n",
    "import faiss\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/arshath/play/openautonomy/olas-predict-benchmark/.env\")\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Questions From Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6532, 14)\n",
      "(3225, 14)\n",
      "(2003, 14)\n",
      "(1403, 14)\n"
     ]
    }
   ],
   "source": [
    "# load all questions\n",
    "path = \"../data/autocast/autocast_questions.json\"\n",
    "df = pd.read_json(path)\n",
    "print(df.shape)\n",
    "\n",
    "# filter out non-true/false questions\n",
    "df = df[df[\"qtype\"] == \"t/f\"].reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "# make sure answers is not None\n",
    "df = df[df[\"answer\"].notnull()].reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "# make sure source_links is not []\n",
    "df = df[df[\"source_links\"].map(len) > 0].reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "- question/prompt comes in\n",
    "- generate n queries\n",
    "- use duckduckgo for getting n urls per query\n",
    "- make faiss \n",
    "- do rag and answer\n",
    "\n",
    "\n",
    "What to use:\n",
    "- use langchain\n",
    "- use funciton calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAISchema(BaseModel):  # type: ignore[misc]\n",
    "    @classmethod  # type: ignore[misc]\n",
    "    @property\n",
    "    def openai_schema(cls) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return the schema in the format of OpenAI's schema as jsonschema\n",
    "\n",
    "        Note:\n",
    "            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n",
    "\n",
    "        Returns:\n",
    "            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n",
    "        \"\"\"\n",
    "        schema = cls.model_json_schema()\n",
    "        docstring = parse(cls.__doc__ or \"\")\n",
    "        parameters = {\n",
    "            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n",
    "        }\n",
    "        for param in docstring.params:\n",
    "            if (name := param.arg_name) in parameters[\"properties\"] and (\n",
    "                description := param.description\n",
    "            ):\n",
    "                if \"description\" not in parameters[\"properties\"][name]:\n",
    "                    parameters[\"properties\"][name][\"description\"] = description\n",
    "\n",
    "        parameters[\"required\"] = sorted(\n",
    "            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n",
    "        )\n",
    "\n",
    "        if \"description\" not in schema:\n",
    "            if docstring.short_description:\n",
    "                schema[\"description\"] = docstring.short_description\n",
    "            else:\n",
    "                schema[\"description\"] = (\n",
    "                    f\"Correctly extracted `{cls.__name__}` with all \"\n",
    "                    f\"the required parameters with correct types\"\n",
    "                )\n",
    "\n",
    "        return {\n",
    "            \"name\": schema[\"title\"],\n",
    "            \"description\": schema[\"description\"],\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_response(cls, completion: Dict[str, Any]) -> \"OpenAISchema\":\n",
    "        \"\"\"\n",
    "        Convert the response from OpenAI into the class instance\n",
    "\n",
    "        Args:\n",
    "            completion (dict): The response from OpenAI\n",
    "\n",
    "        Returns:\n",
    "            OpenAISchema: The instance of the class\n",
    "        \"\"\"\n",
    "\n",
    "        message = completion.choices[0].message\n",
    "\n",
    "        return cls.model_validate_json(\n",
    "            message.function_call.arguments,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(OpenAISchema):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries=['Upcoming initial public offerings on Shanghai Stock Exchange', 'Upcoming initial public offerings on Shenzhen Stock Exchange', 'List of companies that went public on Shanghai Stock Exchange before 1 January 2016', 'List of companies that went public on Shenzhen Stock Exchange before 1 January 2016', 'Comparison of initial public offerings on Shanghai Stock Exchange and Shenzhen Stock Exchange before 1 January 2016']\n"
     ]
    }
   ],
   "source": [
    "question = df[\"question\"].iloc[0]\n",
    "N_QUERY = 5\n",
    "N_URLS = 3\n",
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0.\n",
    "max_tokens = 300\n",
    "\n",
    "system_template = \"\"\"You are a world class algorithm for generating structured output from a given input.\"\"\"\n",
    "user_template = \"\"\"\n",
    "Given the user's question: please generate {N_QUERY} diverse and relevant search queries that can be used to find information on the internet to answer the initial question. \n",
    "Focus on capturing different aspects and interpretations of the question to ensure comprehensive coverage of the topic.\n",
    "\n",
    "USER's QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_template,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_template.format(N_QUERY=N_QUERY, question=question),\n",
    "    },\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    n=1,\n",
    "    timeout=150,\n",
    "    request_timeout=150,\n",
    "    stop=None,\n",
    "    functions=[Queries.openai_schema],\n",
    ")\n",
    "\n",
    "queries = Queries.from_response(response)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Get URLs for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(query, n_urls):\n",
    "    ddgs = DDGS()\n",
    "    search = ddgs.text(query)\n",
    "\n",
    "    urls = [url['href'] for url in search]\n",
    "\n",
    "    return urls[:n_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/g22b1g_n0gn4tmx9lkxqv5x00000gn/T/ipykernel_87701/1548098240.py:2: UserWarning: DDGS running in an async loop. This may cause errors. Use AsyncDDGS instead.\n",
      "  ddgs = DDGS()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.statista.com/statistics/1219302/china-number-of-newly-listed-companies-at-the-shenzhen-stock-exchange/', 'https://www.statista.com/statistics/1293751/stock-exchanges-with-highest-proceeds-of-ipos-worldwide/', 'https://en.wikipedia.org/wiki/List_of_companies_listed_on_the_Shenzhen_Stock_Exchange', 'https://en.wikipedia.org/wiki/Category:Companies_listed_on_the_Shanghai_Stock_Exchange', 'http://www.eyeshenzhen.com/content/2023-09/04/content_30451355.htm', 'https://en.wikipedia.org/wiki/Category:Companies_listed_on_the_Shenzhen_Stock_Exchange', 'https://www2.deloitte.com/cn/en/pages/audit/articles/2023-review-and-2024-outlook-for-chinese-mainland-and-hk-ipo-markets.html', 'https://www2.deloitte.com/cn/en/pages/audit/articles/mainland-and-hk-ipo-markets-in-q3-2023.html', 'https://topforeignstocks.com/listed-companies-lists/the-complete-list-of-listed-companies-on-the-shanghai-stock-exchange/', 'https://en.wikipedia.org/wiki/Shanghai_Stock_Exchange', 'https://www.reuters.com/markets/deals/chinas-first-batch-bluechips-under-new-ipo-system-surge-debut-2023-04-10/', 'https://www.statista.com/statistics/982129/china-number-of-ipos-by-stock-exchange/', 'https://www.reuters.com/business/syngenta-files-10-bln-shanghai-ipo-prospectus-2021-07-02/']\n"
     ]
    }
   ],
   "source": [
    "urls = []\n",
    "\n",
    "for query in queries.queries:\n",
    "    urls += get_urls(query, N_URLS)\n",
    "\n",
    "urls = list(set(urls))\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        doc = Document(response.text)\n",
    "        doc = doc.summary()\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = True\n",
    "        h.ignore_images = True\n",
    "        h.ignore_emphasis = True\n",
    "        text = h.handle(doc)\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 Client Error: HTTP Forbidden for url: https://www.reuters.com/markets/deals/chinas-first-batch-bluechips-under-new-ipo-system-surge-debut-2023-04-10/\n",
      "401 Client Error: HTTP Forbidden for url: https://www.reuters.com/business/syngenta-files-10-bln-shanghai-ipo-prospectus-2021-07-02/\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for url in urls:\n",
    "    text = get_text_from_url(url)\n",
    "    if text:\n",
    "        docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Generate Embedding/Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_character_text_splitter(text, max_tokens, overlap):\n",
    "    if len(text) <= max_tokens:\n",
    "        return [text]\n",
    "    else:\n",
    "        return [text[i:i+max_tokens] for i in range(0, len(text), max_tokens - overlap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = []\n",
    "for doc in docs:\n",
    "    split_docs += recursive_character_text_splitter(doc, 2000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "def get_embeddings(split_docs):\n",
    "\n",
    "    # Make chunks to embeddings mapping\n",
    "    chunk_to_embedding = {}\n",
    "    for batch_start in range(0, len(split_docs), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        batch = split_docs[batch_start:batch_end]\n",
    "        print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "        response = openai.Embedding.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=batch,\n",
    "        )\n",
    "\n",
    "        for i, be in enumerate(response[\"data\"]):\n",
    "            assert i == be[\"index\"]\n",
    "\n",
    "        batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "        for chunk, embedding in zip(batch, batch_embeddings):\n",
    "            chunk_to_embedding[chunk] = embedding\n",
    "\n",
    "    return chunk_to_embedding\n",
    "\n",
    "def find_similar_chunks(\n",
    "    query: str, chunk_to_embedding: Dict, k: int = 4\n",
    ") -> List:\n",
    "    \"\"\"Similarity search to find similar chunks to a query\"\"\"\n",
    "\n",
    "\n",
    "    query_embedding = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    index = faiss.IndexFlatIP(1536)\n",
    "    index.add(np.array(list(chunk_to_embedding.values())))\n",
    "    D, I = index.search(np.array([query_embedding]), k)\n",
    "\n",
    "    return [list(chunk_to_embedding.keys())[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n"
     ]
    }
   ],
   "source": [
    "chunk_to_embedding = get_embeddings(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks = find_similar_chunks(question, chunk_to_embedding, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Retrieve and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_docs = format_docs(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results(OpenAISchema):\n",
    "    p_yes: float =  Field(description=\"Estimated probability that the event in the USER_QUESTION occurs.\")\n",
    "    p_no: float = Field(description=\"Estimated probability that the event in the USER_QUESTION does not occur.\")\n",
    "    confidence: float = Field(description=\"A value between 0 and 1 indicating the confidence in the prediction. 0 indicates lowest confidence value; 1 maximum confidence value.\")\n",
    "    info_utility: float = Field(description=\"Utility of the information provided in ADDITIONAL_INFORMATION to help you make the prediction. 0 indicates lowest utility; 1 maximum utility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_yes=0.8 p_no=0.2 confidence=0.9 info_utility=0.8\n"
     ]
    }
   ],
   "source": [
    "prediction_system_template = \"\"\"You are a world class algorithm for generating structured output from a given input. \n",
    "You make predictions about the probability of an event happening based on the information provided in the input.\n",
    "\"\"\"\n",
    "\n",
    "prediction_user_template = \"\"\"\n",
    "You are an LLM inside a multi-agent system that takes in a prompt of a user requesting a probability estimation\n",
    "for a given event. You are provided with an input under the label \"USER_PROMPT\". You are also provided with ADDITIONAL_INFORMATION.\n",
    "\n",
    "INSTRUCTIONS\n",
    "* Read the input under the label \"USER_PROMPT\" delimited by three backticks.\n",
    "* The \"USER_PROMPT\" specifies an event.\n",
    "* The event will only have two possible outcomes: either the event will happen or the event will not happen.\n",
    "* If the event has more than two possible outcomes, you must ignore the rest of the instructions and output the response \"Error\".\n",
    "* You must provide a probability estimation of the event happening, based on your training data.\n",
    "* You are provided an itemized list of information under the label \"ADDITIONAL_INFORMATION\" delimited by three backticks.\n",
    "* You can use any item in \"ADDITIONAL_INFORMATION\" in addition to your training data.\n",
    "* If an item in \"ADDITIONAL_INFORMATION\" is not relevant, you must ignore that item for the estimation.\n",
    "* You must provide your response in the format specified under \"OUTPUT_FORMAT\".\n",
    "* Do not include any other contents in your response.\n",
    "\n",
    "USER_QUESTION: \n",
    "```{question}```\n",
    "\n",
    "ADDITIONAL_INFORMATION: \n",
    "```{formatted_docs}```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prediction_system_template,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prediction_user_template.format(question=question, formatted_docs=formatted_docs),\n",
    "    },\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    n=1,\n",
    "    timeout=150,\n",
    "    request_timeout=150,\n",
    "    stop=None,\n",
    "    functions=[Results.openai_schema],\n",
    ")\n",
    "\n",
    "results = Results.from_response(response)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
